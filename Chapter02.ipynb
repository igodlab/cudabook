{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89def4f7-386c-404c-b5b7-0747cd85da75",
   "metadata": {},
   "source": [
    "# 2 Heterogeneous data parallel computing\n",
    "\n",
    "- *Data parallelism* - refers to the phenomenon in which different parts of the dataset can be computed independently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830bdf9-7952-4861-b435-1337d6feff98",
   "metadata": {},
   "source": [
    "## 2.1 Data parallelism\n",
    "\n",
    "- We'll work alongside an example to elaborate on this topic. Lets consider image manipulation, where we handle millions to trillions of pixels\n",
    "    - eg. to convert a colored image to grayscale we need to compute the luminosity to the three channel matrices: $L=0.21r+0.72g+0.07b$, for all $N-1$ pixels: $O[0]=L(I[0](r,g,b)),\\ldots, O[N-1]=L(I[N-1](r,g,b))$\n",
    "- > **Task Parallelism vs Data Parallelism** - in general, the former is the main source of scalability but not the only type of paralellism. The latter also gives room for sqeezing parllelizable performance and a nice detail is the larger the application, the larger the independent tasks present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e910bf9-2895-47c4-941c-466986c404f6",
   "metadata": {},
   "source": [
    "## 2.2 CUDA C program structure\n",
    "\n",
    "- CUDA C is NVIDIA's programming language that unlocks access to heterogeneous computing systems composed by CPU cores and massively parallel GPUs\n",
    "    - CUDA C extends ANSI C with minimal new syntax and libraries (plus some C++ features) to target heterogenous computing\n",
    "    - CUDA C's code structure reflects the structure of a *host* (CPU) and *devices* (GPUs) in a computer.\n",
    "- Fig.2.3 shows the simplified (CPU threads don't overlap w/ GPU threads) scheme of the execution of *grids* \n",
    "    - In the color-to-grayscale example each thread will be used to compute one output pixel, so we can expect $N-1$ threads to be generated and scheduled. These take very few clock cycles in contrast to CPU threads which take thousands of clock cycles to generate and schedule\n",
    "      \n",
    "<img src=\"images/ch022-cuda-program.png\" width=\"60%\">\n",
    "      \n",
    "- > **Threads** - are a simplified view of how a processor executes a sequential program in a coputer. Contains the code of the program, the point in the code that is being executed and the values of its variables and data structures. Threads are sequential, even in CUDA programs, where a program initiates parallel execution by calling kernel functions which launches grids of execution (through its underlying runtime mechanisms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de87497-8f5a-45e1-896b-446f6f33e13d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.3 A vector addition kernel\n",
    "\n",
    "- Lets walk through the \"Hello World\" equivalent example for sequential programming ie. vector addition. *Notation.-* host variables will always have `_h` whereas variables used by the device will have `_d`\n",
    "    - Traditional vector addition (Fig.2.4) we initialize & allocate memory in `main` (skipped) and then compute everything on host; in parallelized vector addition (Fig.2.5) - in Part 1 we allocate memory and copy vectors to device, in Part 2 we launch the grid of threads to compute element-wise addition and in Part 3 we copy C back to host and remove all vectors in device\n",
    "      \n",
    "<img src=\"images/ch023-vector-sum.png\" width=\"80%\">\n",
    "      \n",
    "- > **Pointers in C lang** - regular (pointer) variables are declared as `float V` (`float *P`). We can make `P` access the value of `V` w/ `P=&V`. So the args for `vecAdd` are pointers that access the i-th element of `A_h, B-h, C_h`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc5f45-51c3-4390-b340-7457586016ad",
   "metadata": {},
   "source": [
    "## 2.4 Device global memory and data transfer\n",
    "\n",
    "- CUDA hardware devices come with its own random-access memory called *device global memory* aka. *global memory* which is different than other memory components of a computer\n",
    "- We've seen that computing the parallelized version of vector addition kernel (Fig.2.5) requires to transfer objects from the host's memory to global memory and back (and free up memory after computations).\n",
    "- This is can be done thanks to **CUDA C runtime system** which offers APIs for the programmer to perform these activities eg. two common ones are:\n",
    "\n",
    "```c\n",
    "    cudaError_t cudaMalloc(void **  devPtr, size_t size)\t\t\t\n",
    "    /* Allocates size bytes of linear memory on the device and returns in *devPtr a pointer to the allocated memory. The allocated memory is suitably aligned for any kind of variable. The memory is not cleared. cudaMalloc() returns cudaErrorMemoryAllocation in case of failure.\n",
    "    \n",
    "    Parameters:\n",
    "    devPtr \t- Pointer to allocated device memory\n",
    "    size \t- Requested allocation size in bytes\n",
    "    Returns:\n",
    "    cudaSuccess, cudaErrorMemoryAllocation */\n",
    "    \n",
    "    \n",
    "    cudaError_t cudaFree(void * devPtr) \t\n",
    "    /* Frees the memory space pointed to by devPtr, which must have been returned by a previous call to cudaMalloc() or cudaMallocPitch(). Otherwise, or if cudaFree(devPtr) has already been called before, an error is returned. If devPtr is 0, no operation is performed. cudaFree() returns cudaErrorInvalidDevicePointer in case of failure.\n",
    "    \n",
    "    Parameters:\n",
    "    devPtr \t- Device pointer to memory to free\n",
    "    Returns:\n",
    "    cudaSuccess, cudaErrorInvalidDevicePointer, cudaErrorInitializationError */\n",
    "```\n",
    "\n",
    "- > CUDA C uses the standard C runtime library `malloc` function to manage the host memory (thus the similarity between `cudaMalloc` and C's `malloc`). Moreover, the fst arg for `cudaMalloc(void **  devPtr, ...)` has a double `**` ie. **address** to a pointer variable which allows to cast **any type** of object to global memory allocation\n",
    "\n",
    "```c\t\n",
    "    cudaError_t cudaMemcpy (void * dst, const void * src, size_t count, enum cudaMemcpyKind kind)\t\t\t\n",
    "    /*Copies count bytes from the memory area pointed to by src to the memory area pointed to by dst, where kind is one of cudaMemcpyHostToHost, cudaMemcpyHostToDevice, cudaMemcpyDeviceToHost, or cudaMemcpyDeviceToDevice, and specifies the direction of the copy. The memory areas may not overlap. Calling cudaMemcpy() with dst and src pointers that do not match the direction of the copy results in an undefined behavior.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    dst \t- Destination memory address\n",
    "    src \t- Source memory address\n",
    "    count \t- Size in bytes to copy\n",
    "    kind \t- Type of transfer\n",
    "    Returns:\n",
    "    cudaSuccess, cudaErrorInvalidValue, cudaErrorInvalidDevicePointer, cudaErrorInvalidMemcpyDirection */\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf578cf-515e-428f-ad5f-311b63c03a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3872c6-1c9c-4ec1-b4c1-180ef69c5a63",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "\n",
    "- One of the main ageless goals in Computer Science is to accelerate computing power ie. make processors go brrr\n",
    "- The history of this battle goes back to the XX century where the first processor designs relied on a *Central Processing Unit (CPU)* hardware component that is capable of executing programs step by step (the [von Newmann et. al 1972](https://ieeexplore.ieee.org/abstract/document/238389) design based on a program counter for sequential program execution aka. *thread*). The story highlights go as follows:\n",
    "    - **Single Microprocessor age** - progressively increased clock speeds of a single CPU (1980s-1990s). Brought GFLOPS (TFLOPS) to desktops (data centers)\n",
    "    - **Megahertz wars** - reached a barrier trying to increase clock freq (<2003) due to energy dissipation and power supply limits\n",
    "    - **Concurrency revolution** - overcame the limit ([Sutter and Larus, >2005](https://dl.acm.org/doi/10.1145/1095408.1095421)) with *multi processor cores* where *parallel* programs execute multiple threads cooperatively to get the work done faster\n",
    "    - **Inception of GPUs** - invention of a different architecture that is throughput-oriented (2006) to be combined with CPUs. Very impractical to use because the way of giving it instructions was through API-like General Purpose programming of GPU (GPGPU)\n",
    "    - **[NVIDIA, 2007](https://dl.acm.org/doi/10.1145/1095408.1095421) gives birth to CUDA** - Market demand in gaming justified the commercial developement of GPUs plus (2007). NVIDIA's innovation was to build a silicon interface which serves the requests of *Compute Unified Architecture (CUDA)* programs, so GPGPUs wouldn't talk to the graphic interface at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0473d-6b07-4f06-b449-0d52fb323522",
   "metadata": {},
   "source": [
    "## 1.1 Heterogeneous parallel computing\n",
    "\n",
    "- The industry settled on two non mutually-exclusive trajectories for designing microprocessors ([Hwu et al., 2008](https://www.scopus.com/pages/publications/49549087268)):\n",
    "    - *Multicore trajectory* - seeks to maintain the execution speed of sequential programs while moving to multiple cores (eg. Intel's 24 *out-of-order* multicore supporting the full $\\times 86$ instruction set. Or the ARM Ampere w/ 128 multicore)\n",
    "    - *Many-thread trajectory* - focuses on execution throughput of parallel applications (eg. NVIDIA's Tesla 100 GPU w/ many 10k threads executing *in-order* pipelines)\n",
    "- GPUs excel at floating-point operations, being capable of $\\sim 30$ ($\\sim 230$) times more throughput than CPUs at double-precision (single-precision)\n",
    "    - eg. peak throughput of the A100 is 9.7 TFLOPS (64-bit double-precision), 156 TFLOPS (32-bit single-precision), 312 TFLOPS (16-bit half-precision) whereas the Intel 24-core outputs 0.33 TFLOPS (double-precision) and 0.66 TFLOPS (single-precision)\n",
    "- More and more applications developers have moved parts of their applications to run on GPUs due to the advantageous throughput gap. The philosophy that holds is \"when there is more work to do, there is more opportunity to divide the work among cooperating parallel workers ie. *threads*\"\n",
    "    - The peak performance gap exists due to the differences in design where CPUs are optimized for latency. Whereas GPUs are optimized for throughput\n",
    "    - It is more expensive to reduce latency than increase throughput.\n",
    "- *Heterogeneus parallel computing* refers to non-homogeneus computing components ie. CPU-GPU (also field-programmable arrays for networking applications as well) and parallel refers to multicore parallel programs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f76c44b-7548-4496-b797-205efcf83f2d",
   "metadata": {},
   "source": [
    "## 1.2 Why more speed or parallelism\n",
    "\n",
    "- Although, many applications run satisfactorily fast today the goal of the processor industry is to keep improving hardware for applications of the future eg. in fields like: Molecular Biology, TV/gaming/entertainment & Aritificial Intelligence (the most evident one)\n",
    "- The goal of this book summary is to teach programming models that facilitate parallel implementations of data management and delivery (through CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0340a97a-5e05-48d6-8a40-650d11160094",
   "metadata": {},
   "source": [
    "## 1.3 Speeding up real applications\n",
    "\n",
    "- Fortunately most parts of modern applications are candidates for *parallelization optimization* (basically everything excluding the sequential parts of an application where CPUs are the undisputed in-charge parts). The peach analogy is good for exemplifying this\n",
    "\n",
    "<img src=\"images/ch013-peach.png\" width=\"45%\">\n",
    "  \n",
    "- The definition of *speedup* for an application by computing system A over computing system B is the ratio of execution times: $t^B_{\\text{exec}}/t^A_{\\text{exec}}$\n",
    "    - The speedup that is achievable by a parallel computing system over a serial computing system depends on the portion of the application that can be parallelized. Given by [Amdahl law](https://en.wikipedia.org/wiki/Amdahl's_law): $\\text{Speedup}_\\text{overall}=\\frac{1}{(1 - t_{\\text{optimized}}) + \\frac{t_{\\text{optimized}}}{\\text{Speedup}_{\\text{optimized}}}}$\n",
    "        -  eg. if a program's parallelizable part is $30\\%$ a $100\\times$ speedup of it would result is no more than $\\frac{1}{(1 - 0.3) + \\frac{0.3}{100}}=1.42\\times$\n",
    "        - see plotly Figure for a bigger picture of Amdahl law\n",
    "    - Speedup doesn't depend only on *parallelizable optimization* but aslo on how fast we data can be accessed and written to the memory *bandwidth limitations*\n",
    "        - the trick is to bypass memory limitations by applying transformations to utilize GPU memories to reduce number of accesses to the DRAM\n",
    "\n",
    "<img src=\"images/ch013-amdahl-law.png\" width=\"45%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05d73b-39c6-45de-89ad-b9cd615ea9d9",
   "metadata": {},
   "source": [
    "## 1.4 Challenges in parallel programming\n",
    "\n",
    "- We wouldn't care about parallel programs if we didn't care about performance! But we do!\n",
    "- Future Chapters deal with introducing nonintuitive ways of writing parallel programs, because many solutions are described in terms of *mathematical recurrences*. Some algo primitives are\n",
    "    - *Prefix sum* - facilitates the conversion of sequential, recursive formulations into a more parallel one (Chapter 11)\n",
    "    - *Work efficiency* - gives a metric that informs about the tradeoffs or parallelizing programs\n",
    "    - *Memory-bound (& compute-bound) applications* - which speed is limited by memory access latency and/or throughput (number of instructions performed per byte of data) (Chapters 5, 6)\n",
    "    - *Input data drawbacks* - irregularities in input data such as variable data sizes and uneven data distributions decrease effectiveness of parallel programs due to uneven amounts of work assigned to threads\n",
    "    - *Embarassingly parallel* applications can be parallelized with little collaboration between threads. Their opposite are applications that need a lot of collaboration between threads require *synchronization techniques* eg. barriers & atomic operations\n",
    "- Fortunately most of these challenges have been addressed by researchers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f279e3-e8ea-4980-bf3e-d031b14bac9f",
   "metadata": {},
   "source": [
    "## 1.5 Related parllel programming interfaces\n",
    "\n",
    "- Most significant parallel programming languages and interfaces:\n",
    "    - **OpenMP ([Open, 2005](https://dl.acm.org/doi/10.1145/1095408.1095421))**\n",
    "    - **Message Passing Interface ([MPI, 2009](https://www.mpi-forum.org/docs/mpi-2.2/mpi22-report.pdf))**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
